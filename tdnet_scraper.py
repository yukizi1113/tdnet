# -*- coding: utf-8 -*-
"""tdnet_scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nL-TGPTPeAjFVauT4qTysZyTrGU5N24H
"""

!pip -q install beautifulsoup4 lxml tqdm

import os
import re
import time
import json
import base64
import hashlib
from urllib.parse import urljoin
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from tqdm.auto import tqdm

# -----------------------
# 設定
# -----------------------
GITHUB_OWNER = "yukizi1113"
GITHUB_REPO  = "tdnet"

TARGET_DATE = "20260105"          # yyyymmdd

PDF_DIR_IN_REPO = "tekigikaizi"
ZIP_DIR_IN_REPO = "XBRL"

OVERWRITE = True

# 除外
#  - ETF: Ｅ－ / E-
#  - REIT: Ｒ－ / R-
#  - ETN: Ｎ－ / N-
EXCLUDE_COMPANY_PREFIXES = ("Ｅ－", "E-", "Ｒ－", "R-", "Ｎ－", "N-")

# アクセス間隔
SLEEP_SEC = 0.12

# -----------------------
# GitHub token
# -----------------------
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
if not GITHUB_TOKEN:
    from getpass import getpass
    GITHUB_TOKEN = getpass("Paste your GitHub token (input hidden): ").strip()

# -----------------------
# 基本
# -----------------------
TDNET_BASE = "https://www.release.tdnet.info/inbs/"
MAIN_URL = urljoin(TDNET_BASE, "I_main_00.html")

UA = "Mozilla/5.0 (compatible; tdnet-downloader/4.3-safe; +https://example.invalid)"

# br(Brotli)を要求しない
TDNET_HEADERS = {
    "User-Agent": UA,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.8,en;q=0.6",
    "Accept-Encoding": "gzip, deflate",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
    "Referer": MAIN_URL,
}

FW2HW_DIGITS = str.maketrans("０１２３４５６７８９", "0123456789")
FW2HW_ALPHA  = str.maketrans(
    "ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ",
    "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
)

SESSION = requests.Session()
SESSION.headers.update({"User-Agent": UA})

# -----------------------
# GitHub API
# -----------------------
def gh_headers(token: str) -> dict:
    return {
        "Authorization": f"Bearer {token}",
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
        "User-Agent": "tdnet-to-github-colab",
    }

def gh_get_content(owner: str, repo: str, path: str, token: str):
    url = f"https://api.github.com/repos/{owner}/{repo}/contents/{path}"
    return SESSION.get(url, headers=gh_headers(token), timeout=60)

def gh_put_file(owner: str, repo: str, path: str, token: str, content_bytes: bytes, message: str,
                overwrite: bool = False):
    url = f"https://api.github.com/repos/{owner}/{repo}/contents/{path}"

    sha = None
    r0 = gh_get_content(owner, repo, path, token)
    if r0.status_code == 200:
        if not overwrite:
            return {"skipped": True, "reason": "exists", "path": path}
        sha = r0.json().get("sha")
    elif r0.status_code != 404:
        raise RuntimeError(f"GitHub precheck failed: {r0.status_code} {r0.text[:300]}")

    payload = {
        "message": message,
        "content": base64.b64encode(content_bytes).decode("utf-8"),
    }
    if sha:
        payload["sha"] = sha

    r = SESSION.put(url, headers=gh_headers(token), data=json.dumps(payload), timeout=120)
    if r.status_code not in (200, 201):
        raise RuntimeError(f"GitHub upload failed: {r.status_code} {r.text[:500]}")
    return {"skipped": False, "path": path, "status": r.status_code}

# -----------------------
# 文字ユーティリティ
# -----------------------
def normalize_ws(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def normalize_text(s: str) -> str:
    return normalize_ws((s or "").translate(FW2HW_DIGITS).translate(FW2HW_ALPHA).replace("　", " "))

def sanitize_filename(name: str, max_len: int = 180) -> str:
    name = normalize_text(name)
    name = re.sub(r'[\\/:*?"<>|]+', "_", name)
    name = normalize_ws(name)
    if len(name) <= max_len:
        return name
    h = hashlib.sha1(name.encode("utf-8")).hexdigest()[:8]
    keep = max_len - (len(h) + 2)
    return f"{name[:keep]}__{h}"

def sniff_ext_from_bytes(b: bytes) -> str:
    if b.startswith(b"%PDF"):
        return ".pdf"
    if b.startswith(b"PK\x03\x04") or b.startswith(b"PK\x05\x06") or b.startswith(b"PK\x07\x08"):
        return ".zip"
    return ""

# -----------------------
# TDnet HTML取得
# -----------------------
def decode_html_bytes(b: bytes) -> str:
    head = b[:5000].lower()
    m = re.search(br"charset\s*=\s*['\"]?([a-z0-9_\-]+)", head)
    enc = None
    if m:
        enc = m.group(1).decode("ascii", errors="ignore")
    for e in [enc, "utf-8", "cp932"]:
        if not e:
            continue
        try:
            return b.decode(e, errors="replace")
        except Exception:
            pass
    return b.decode("utf-8", errors="replace")

def fetch_html(url: str, sleep_sec: float = SLEEP_SEC, retry: int = 2):
    last = None
    for k in range(retry + 1):
        try:
            time.sleep(sleep_sec)
            r = SESSION.get(url, headers=TDNET_HEADERS, timeout=60)
            if r.status_code == 404:
                return None
            r.raise_for_status()
            return decode_html_bytes(r.content)
        except Exception as e:
            last = e
            if k == retry:
                break
            time.sleep(0.5 * (k + 1))
    raise RuntimeError(f"fetch_html failed: {url} ({last})")

def iter_list_pages_for_date(date_yyyymmdd: str, max_pages: int = 200):
    for i in range(1, max_pages + 1):
        url = urljoin(TDNET_BASE, f"I_list_{i:03d}_{date_yyyymmdd}.html")
        html = fetch_html(url)
        if html is None:
            break
        yield url, html

def unwrap_to_content_html(html: str, max_depth: int = 5) -> str:
    cur = html
    for _ in range(max_depth):
        soup = BeautifulSoup(cur, "lxml")
        node = soup.find("iframe", src=True) or soup.find("frame", src=True)
        if not node:
            return cur
        src = (node.get("src") or "").strip()
        if not src:
            return cur
        nxt_url = urljoin(TDNET_BASE, src)
        nxt = fetch_html(nxt_url)
        if not nxt:
            return cur
        cur = nxt
    return cur

# -----------------------
# ID抽出
# -----------------------
_RE_PDF_ID = re.compile(r"(\d{18})\.pdf", re.IGNORECASE)
_RE_ZIP_ID = re.compile(r"(\d{18})\.zip", re.IGNORECASE)
_RE_ID18   = re.compile(r"\b(\d{18})\b")

def extract_pdf_id_from_anchor(a) -> str | None:
    if not a:
        return None
    for attr in ("href", "onclick"):
        s = a.get(attr) or ""
        m = _RE_PDF_ID.search(s)
        if m:
            return m.group(1)
        m = re.search(r"/(\d{18})\.pdf", s, re.IGNORECASE)
        if m:
            return m.group(1)
        ids = [x for x in _RE_ID18.findall(s) if not x.startswith("08")]
        if len(ids) == 1:
            return ids[0]
    return None

def extract_xbrl_zip_id_from_anchor(a) -> str | None:
    if not a:
        return None
    for attr in ("href", "onclick"):
        s = a.get(attr) or ""
        m = _RE_ZIP_ID.search(s)
        if m:
            return m.group(1)
        ids = [x for x in _RE_ID18.findall(s) if x.startswith("08")]
        if len(ids) == 1:
            return ids[0]
    return None

def pick_title_anchor_and_title(tr):
    for a in tr.find_all("a"):
        txt = normalize_ws(a.get_text(" ", strip=True))
        if not txt:
            continue
        if txt.upper() == "XBRL":
            continue
        return a, txt
    return None, ""

def pick_xbrl_anchor(tr):
    for a in tr.find_all("a"):
        txt = normalize_ws(a.get_text(" ", strip=True))
        if txt.upper() == "XBRL":
            return a
    return None

# -----------------------
# ticker抽出
# -----------------------
def parse_tdnet_code_to_ticker(code_cell_text: str) -> str | None:
    s = normalize_text(code_cell_text)
    if not s:
        return None
    tok = s.split()[0].strip("()（）[]【】")

    m = re.match(r"^(\d{4})0$", tok)
    if m:
        return m.group(1)

    m = re.match(r"^(\d{3})([A-Za-z])0$", tok)
    if m:
        return (m.group(1) + m.group(2)).upper()

    m = re.match(r"^(\d{4})([A-Za-z])0$", tok)
    if m:
        return (m.group(1) + m.group(2)).upper()

    m = re.match(r"^(\d{4})$", tok)
    if m:
        return m.group(1)

    m = re.match(r"^(\d{3})([A-Za-z])$", tok)
    if m:
        return (m.group(1) + m.group(2)).upper()

    m = re.match(r"^(\d{4})([A-Za-z])$", tok)
    if m:
        return (m.group(1) + m.group(2)).upper()

    m = re.match(r"^(\d{3})([A-Za-z])0\D", tok)
    if m:
        return (m.group(1) + m.group(2)).upper()
    m = re.match(r"^(\d{4})0\D", tok)
    if m:
        return m.group(1)

    return None

def parse_time_from_tds(td_texts):
    for t in td_texts[:10]:
        tt = normalize_text(t)
        if re.fullmatch(r"\d{1,2}:\d{2}", tt):
            return tt
    return ""

def parse_row_by_td_positions(tr):
    tds = tr.find_all("td")
    if not tds:
        return None

    title_a, title = pick_title_anchor_and_title(tr)
    if not title:
        return None

    pdf_id = extract_pdf_id_from_anchor(title_a)
    xbrl_a = pick_xbrl_anchor(tr)
    xbrl_id = extract_xbrl_zip_id_from_anchor(xbrl_a)

    if not pdf_id and not xbrl_id:
        return None

    title_td_idx = None
    for i, td in enumerate(tds):
        if title_a is not None:
            hit = td.find(lambda tag: getattr(tag, "name", None) == "a" and tag is title_a)
            if hit is not None:
                title_td_idx = i
                break
        txt = normalize_ws(td.get_text(" ", strip=True))
        if txt and normalize_ws(title) == txt and txt.upper() != "XBRL":
            title_td_idx = i

    if title_td_idx is None:
        return None

    td_text_list = [normalize_ws(td.get_text(" ", strip=True)) for td in tds]
    tm = parse_time_from_tds(td_text_list)

    company = ""
    code_text = ""
    found = []
    for j in range(title_td_idx - 1, -1, -1):
        txt = normalize_ws(td_text_list[j])
        if not txt:
            continue
        if re.fullmatch(r"\d{1,2}:\d{2}", normalize_text(txt)):
            continue
        found.append(txt)
        if len(found) >= 2:
            break

    if len(found) >= 1:
        company = found[0]
    if len(found) >= 2:
        code_text = found[1]

    ticker = parse_tdnet_code_to_ticker(code_text)

    if not ticker:
        for j in range(title_td_idx - 1, -1, -1):
            txt = normalize_ws(td_text_list[j])
            cand = parse_tdnet_code_to_ticker(txt)
            if cand:
                ticker = cand
                if j + 1 < len(td_text_list) and normalize_ws(td_text_list[j + 1]):
                    company = normalize_ws(td_text_list[j + 1])
                break

    if not ticker or not company:
        return None

    # ETF/REIT/ETN除外
    if company.startswith(EXCLUDE_COMPANY_PREFIXES):
        return None

    return {
        "ticker": ticker,
        "company": company,
        "title": title,
        "time": tm,
        "pdf_id": pdf_id,
        "xbrl_id": xbrl_id,
    }

def parse_list_page(html: str):
    html = unwrap_to_content_html(html, max_depth=5)
    soup = BeautifulSoup(html, "lxml")
    out = []
    for tr in soup.find_all("tr"):
        r = parse_row_by_td_positions(tr)
        if r:
            out.append(r)
    return out

# -----------------------
# Download / Upload
# -----------------------
def download_bytes(url: str, sleep_sec: float = SLEEP_SEC, retry: int = 2) -> bytes:
    last = None
    for k in range(retry + 1):
        try:
            time.sleep(sleep_sec)
            r = SESSION.get(url, headers=TDNET_HEADERS, timeout=120, allow_redirects=True)
            r.raise_for_status()
            return r.content
        except Exception as e:
            last = e
            if k == retry:
                break
            time.sleep(0.5 * (k + 1))
    raise RuntimeError(f"download failed: {url} ({last})")

def build_base_name(ticker: str, company: str, title: str) -> str:
    return sanitize_filename(f"{ticker}_{company}_{title}")

def run(date_yyyymmdd: str, debug_sample: int = 25):
    all_rows = []
    page_count = 0

    for url, html in iter_list_pages_for_date(date_yyyymmdd, max_pages=200):
        page_count += 1
        all_rows.extend(parse_list_page(html))

    merged = {}
    for r in all_rows:
        key = r.get("pdf_id") or f"NO_PDF__{r['ticker']}__{r['company']}__{r['title']}__{r.get('time','')}"
        if key not in merged:
            merged[key] = dict(r)
        else:
            if not merged[key].get("xbrl_id") and r.get("xbrl_id"):
                merged[key]["xbrl_id"] = r["xbrl_id"]
            if not merged[key].get("pdf_id") and r.get("pdf_id"):
                merged[key]["pdf_id"] = r["pdf_id"]

    rows = list(merged.values())

    print(f"Pages fetched: {page_count}")
    print(f"Rows parsed (non-ETF/REIT/ETN): {len(rows)}")

    if debug_sample > 0:
        print("DEBUG sample rows:")
        for r in rows[:debug_sample]:
            print(
                r["ticker"],
                r["company"],
                r["title"][:60],
                f"(pdf_id={r.get('pdf_id')}, xbrl_id={r.get('xbrl_id')})"
            )

    tasks = []
    seen_task = set()
    for r in rows:
        if r.get("pdf_id"):
            k = ("pdf", r["pdf_id"])
            if k not in seen_task:
                tasks.append(("pdf", r, r["pdf_id"]))
                seen_task.add(k)
        if r.get("xbrl_id"):
            k = ("zip", r["xbrl_id"])
            if k not in seen_task:
                tasks.append(("zip", r, r["xbrl_id"]))
                seen_task.add(k)

    print(f"Files to fetch (strict-linked): {len(tasks)}")

    pdf_manifest = {"date": date_yyyymmdd, "root": f"{PDF_DIR_IN_REPO}/{date_yyyymmdd}/", "items": []}
    zip_manifest = {"date": date_yyyymmdd, "root": f"{ZIP_DIR_IN_REPO}/{date_yyyymmdd}/", "items": []}

    uploaded = 0
    skipped = 0
    failed  = 0

    for kind, r, fid in tqdm(tasks, desc="Download & Upload"):
        ticker, company, title, tm = r["ticker"], r["company"], r["title"], r.get("time","")

        ext = ".pdf" if kind == "pdf" else ".zip"
        url = urljoin(TDNET_BASE, f"{fid}{ext}")

        base = build_base_name(ticker, company, title)
        filename = f"{base}{ext}" if kind == "pdf" else f"{base}__XBRL{ext}"

        repo_dir = PDF_DIR_IN_REPO if kind == "pdf" else ZIP_DIR_IN_REPO
        gh_path = f"{repo_dir}/{date_yyyymmdd}/{filename}"

        try:
            data = download_bytes(url, sleep_sec=SLEEP_SEC, retry=2)
        except Exception as e:
            failed += 1
            m = pdf_manifest if kind == "pdf" else zip_manifest
            m["items"].append({
                "ticker": ticker, "company": company, "title": title, "time": tm,
                "file_id": fid, "source_url": url, "github_path": gh_path,
                "status": "download_failed",
                "error": str(e)[:250],
            })
            continue

        real_ext = sniff_ext_from_bytes(data)
        if (kind == "pdf" and real_ext != ".pdf") or (kind == "zip" and real_ext != ".zip"):
            failed += 1
            m = pdf_manifest if kind == "pdf" else zip_manifest
            m["items"].append({
                "ticker": ticker, "company": company, "title": title, "time": tm,
                "file_id": fid, "source_url": url, "github_path": gh_path,
                "status": "type_mismatch",
                "error": f"expected {ext} but magic bytes look like {real_ext or 'unknown'}",
            })
            continue

        if len(data) > 100 * 1024 * 1024:
            failed += 1
            m = pdf_manifest if kind == "pdf" else zip_manifest
            m["items"].append({
                "ticker": ticker, "company": company, "title": title, "time": tm,
                "file_id": fid, "source_url": url, "github_path": gh_path,
                "status": "skipped_too_large",
                "error": "file > 100MB (GitHub Contents API limit)",
            })
            continue

        try:
            res = gh_put_file(
                GITHUB_OWNER, GITHUB_REPO, gh_path, GITHUB_TOKEN, data,
                message=f"TDnet {date_yyyymmdd} {ticker} {company}: {title} ({fid})",
                overwrite=OVERWRITE
            )
            time.sleep(0.35)
        except Exception as e:
            failed += 1
            m = pdf_manifest if kind == "pdf" else zip_manifest
            m["items"].append({
                "ticker": ticker, "company": company, "title": title, "time": tm,
                "file_id": fid, "source_url": url, "github_path": gh_path,
                "status": "upload_failed",
                "error": str(e)[:250],
            })
            continue

        if res.get("skipped"):
            skipped += 1
            status = "skipped_exists"
        else:
            uploaded += 1
            status = "uploaded"

        m = pdf_manifest if kind == "pdf" else zip_manifest
        m["items"].append({
            "ticker": ticker, "company": company, "title": title, "time": tm,
            "file_id": fid, "source_url": url, "github_path": gh_path,
            "status": status,
        })

    pdf_manifest_path = f"{PDF_DIR_IN_REPO}/{date_yyyymmdd}/manifest.json"
    gh_put_file(
        GITHUB_OWNER, GITHUB_REPO, pdf_manifest_path, GITHUB_TOKEN,
        json.dumps(pdf_manifest, ensure_ascii=False, indent=2).encode("utf-8"),
        message=f"Add PDF manifest {date_yyyymmdd}",
        overwrite=True
    )

    zip_manifest_path = f"{ZIP_DIR_IN_REPO}/{date_yyyymmdd}/manifest.json"
    gh_put_file(
        GITHUB_OWNER, GITHUB_REPO, zip_manifest_path, GITHUB_TOKEN,
        json.dumps(zip_manifest, ensure_ascii=False, indent=2).encode("utf-8"),
        message=f"Add ZIP manifest {date_yyyymmdd}",
        overwrite=True
    )

    print("Done.")
    print(f"uploaded={uploaded}, skipped={skipped}, failed={failed}")
    print(f"PDF manifest: {pdf_manifest_path}")
    print(f"ZIP manifest: {zip_manifest_path}")

# 実行
run(TARGET_DATE, debug_sample=25)